{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Workstation\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# path_to_prototype = '../'\n",
    "import pandas as pd\n",
    "import sys\n",
    "# sys.path.append(path_to_prototype)\n",
    "import pickle as pkl\n",
    "from __future__ import print_function\n",
    "from IKOC import IKOC\n",
    "from IKOC import KOC\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from ipyparallel import Client\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from functools import partial\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "### For AUC Calculation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tot_dat = pd.read_csv('heart_data.csv', index_col=0)\n",
    "labels = pd.read_csv('heart_labels.csv', index_col=0)\n",
    "priv_dat = pd.read_csv('priv_heart_data.csv', index_col=0)\n",
    "\n",
    "\n",
    "# print tot_dat.shape, labels.shape, priv_dat.shape\n",
    "# target_files = labels.query('Class == {}'.format(index)).index\n",
    "# target_labels = labels.ix[target_files].Class == index\n",
    "\n",
    "# test_indices = set(feature_space.index) - set(target_files) ### in Set for\n",
    "# test_indices = pd.Index(test_indices)\n",
    "# Outlier_labels = labels.ix[test_indices].Class == index\n",
    "# print Outlier_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalized the features\n",
    "normalized = StandardScaler().fit_transform(tot_dat)\n",
    "feature_space = pd.DataFrame(normalized, \n",
    "                             index=tot_dat.index, \n",
    "                             columns=tot_dat.columns)\n",
    "\n",
    "normalized = StandardScaler().fit_transform(priv_dat)\n",
    "privileged_space_tot = pd.DataFrame(normalized,\n",
    "                               index=priv_dat.index,\n",
    "                               columns=priv_dat.columns)\n",
    "\n",
    "#### Select which feature would be selected as Privileged Feature and remove this feature from the feature space\n",
    "#### (Privileged feature removal part is in the next cell) \n",
    "\n",
    "### Select sex as privileged attribute i.e p1\n",
    "# privileged_space = privileged_space_tot.ix[:]['p1']\n",
    "\n",
    "### Select age as privileged attribute i.e p2\n",
    "privileged_space = privileged_space_tot.ix[:]['p2']\n",
    "\n",
    "### Select elect as privileged attribute i.e p3\n",
    "# privileged_space = privileged_space_tot.ix[:]['p3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary KOC(KRR-based One-class Classification) or OneClassELM with Thr1 i.e. some % of rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Just remove the group attribute because we are treating it as a privileged information\n",
    "\n",
    "### Drop sex because it is privileged attribute as p1\n",
    "# feature_space = feature_space.drop('a2', axis=1)\n",
    "# privfeat = 'Sex'\n",
    "\n",
    "### Drop age because it is privileged attribute as p2\n",
    "feature_space = feature_space.drop('a1', axis=1)\n",
    "privfeat = 'Age'\n",
    "\n",
    "### Drop elect because it is privileged attribute as p3\n",
    "# feature_space = feature_space.drop('a7', axis=1)\n",
    "# privfeat = 'Elect'\n",
    "\n",
    "# feature_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Workstation\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:80: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 4 0.70 10000000000.00"
     ]
    }
   ],
   "source": [
    "all_res_val = {}\n",
    "all_res_test = {}\n",
    "X_train = {}\n",
    "X_testN = {}\n",
    "y_train = {}\n",
    "y_testN = {}\n",
    "X_val = {}  \n",
    "X_testO = {}\n",
    "y_val = {}\n",
    "y_testO = {} \n",
    "\n",
    "##### Either run the commented lines after first for loop to generate 5-fold indices or load the saving indices as below\n",
    "with open('heart_5fold_index.pkl', 'rb') as f:\n",
    "    X_train, X_testN, y_train, y_testN, X_val, X_testO, y_val, y_testO = pkl.load(f)\n",
    "        \n",
    "for index in range(1, 3):\n",
    "    \n",
    "        ##### Either run this to generate 5-fold indices or load the saving indices as before above for loop\n",
    "        \n",
    "#     target_files = labels.query('Class == {}'.format(index)).index   ### All Target Class Files wiith indices\n",
    "#     test_indices = set(feature_space.index) - set(target_files) ### in Set for\n",
    "#     test_indices = pd.Index(test_indices) ### Convert set to index\n",
    "#     target_labels = labels.ix[target_files].Class == index ### Target data labels\n",
    "#     Outlier_labels = labels.ix[test_indices].Class == index ### Target data labels\n",
    "\n",
    "    \n",
    "#     kf = KFold(n_splits=5) ### k-fold Cross-Validation\n",
    "#     # For Target Files\n",
    "#     fold = 0\n",
    "#     X_train[index] = {}\n",
    "#     X_testN[index] = {}\n",
    "#     y_train[index] = {}\n",
    "#     y_testN[index] = {}\n",
    "#     for train_index, test_index in kf.split(target_files):\n",
    "#         #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#         X_train[index][str(fold)]=target_files[train_index]\n",
    "#         X_testN[index][str(fold)]=target_files[test_index]\n",
    "#         y_train[index][str(fold)]=target_labels[train_index]\n",
    "#         y_testN[index][str(fold)]=target_labels[test_index]\n",
    "#         fold = fold+1\n",
    "\n",
    "#     # For Outlier Files\n",
    "#     fold = 0\n",
    "#     X_val[index] = {}  \n",
    "#     X_testO[index] = {}\n",
    "#     y_val[index] = {}\n",
    "#     y_testO[index] = {}    \n",
    "#     for train_index, test_index in kf.split(test_indices):\n",
    "# #         print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#         X_val[index][str(fold)]=test_indices[train_index]\n",
    "#         X_testO[index][str(fold)]=test_indices[test_index]\n",
    "#         y_val[index][str(fold)]=Outlier_labels[train_index]\n",
    "#         y_testO[index][str(fold)]=Outlier_labels[test_index] \n",
    "#         fold=fold+1\n",
    "\n",
    "        \n",
    "### Cocatenate and Prepare final data\n",
    "    all_res_val[index] = {}\n",
    "    all_res_test[index] = {}\n",
    "    for f in range(0, 5):\n",
    "#        print index,f\n",
    "        X_vali = pd.concat([feature_space.ix[X_train[index][str(f)]][int(math.floor(len(X_train[index][str(f)])*0.8)):], feature_space.ix[X_val[index][str(f)]]], axis=0,sort=False)\n",
    "        Y_vali = pd.concat([y_train[index][str(f)][int(math.floor(len(X_train[index][str(f)])*0.8)):], y_val[index][str(f)]], axis=0,sort=False)\n",
    "        X_testi = pd.concat([feature_space.ix[X_testN[index][str(f)]], feature_space.ix[X_testO[index][str(f)]]], axis=0,sort=False)\n",
    "        Y_testi = pd.concat([y_testN[index][str(f)], y_testO[index][str(f)]], axis=0,sort=False)\n",
    "#### Take 80% of samples for training and keep 20% samples for validation\n",
    "        train_id = X_train[index][str(f)][:int(math.floor(len(X_train[index][str(f)])*0.8))]\n",
    "        train_data = feature_space.ix[train_id]\n",
    "        train_label = y_train[index][str(f)][:int(math.floor(len(X_train[index][str(f)])*0.8))]\n",
    "        all_nu = np.linspace(0.05, 0.7, 10)\n",
    "        all_gamma = np.logspace(-10, 10, 20) # Power of 10 is called logspace. -5 is start point, 5 is last point ans 20 points will be generated\n",
    "        all_res_val[index][f] = {}\n",
    "        all_res_test[index][f] = {}\n",
    "        for nu in all_nu:\n",
    "            all_res_val[index][f][nu] = {}\n",
    "            all_res_test[index][f][nu] = {}\n",
    "            for gamma in all_gamma:\n",
    "                features_kernel = partial(rbf_kernel, gamma=gamma)\n",
    "                model = KOC(nu, features_kernel=features_kernel)\n",
    "                model.fit(train_data.as_matrix())\n",
    "                pred_val = model.decision_function(X_vali)\n",
    "                all_res_val[index][f][nu][gamma] = pred_val              \n",
    "                pred_test = model.decision_function(X_testi)\n",
    "                all_res_test[index][f][nu][gamma] = pred_test           \n",
    "#                 print '\\r {0} {1} {2:.2f} {3:.2f}'.format(index, f, nu, gamma),\n",
    "                ### Python 3\n",
    "                print ('\\r {0} {1} {2:.2f} {3:.2f}'.format(index, f, nu, gamma), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('heart_5fold_index.pkl', 'wb') as f:\n",
    "#     pkl.dump([X_train, X_testN, y_train, y_testN, X_val, X_testO, y_val, y_testO], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(all_res_val, open('Heart_'+ privfeat +'_resVal_ordinary_1_KOC_5CV_CV.pkl', 'wb')) # wb denotes write bytestream\n",
    "pkl.dump(all_res_test, open('Heart_'+ privfeat +'_resTest_ordinary_1_KOC_5CV_CV.pkl', 'wb')) # wb denotes write bytestream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultsVal = pkl.load(open('Heart_'+ privfeat +'_resVal_ordinary_1_KOC_5CV_CV.pkl', 'rb')) # rb denotes read bytestream\n",
    "resultsTest = pkl.load(open('Heart_'+ privfeat +'_resTest_ordinary_1_KOC_5CV_CV.pkl', 'rb')) # rb denotes read bytestream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 9\n",
      "mean_AUC_PRCurve 0.3379043657891682 0.6286261019634476\n"
     ]
    }
   ],
   "source": [
    "all_nu = np.linspace(0.05, 0.7, 10)\n",
    "all_gamma = np.logspace(-10, 10, 20) \n",
    "\n",
    "val_scores = np.zeros((2,5, all_nu.size, all_gamma.size))\n",
    "test_scores = np.zeros((2,5, all_nu.size, all_gamma.size))\n",
    "\n",
    "###    Load 5-fold CV files\n",
    "with open('heart_5fold_index.pkl', 'rb') as f:\n",
    "    X_train, X_testN, y_train, y_testN, X_val, X_testO, y_val, y_testO = pkl.load(f)\n",
    "    \n",
    "for index_index, index in enumerate(range(1, 3)):\n",
    "    for f_index, f in enumerate(range(0, 5)):\n",
    "        \n",
    "#        print index,f\n",
    "        X_vali = pd.concat([feature_space.ix[X_train[index][str(f)]][int(math.floor(len(X_train[index][str(f)])*0.8)):], feature_space.ix[X_val[index][str(f)]]], axis=0,sort=False)\n",
    "        Y_vali = pd.concat([y_train[index][str(f)][int(math.floor(len(X_train[index][str(f)])*0.8)):], y_val[index][str(f)]], axis=0,sort=False)\n",
    "        X_testi = pd.concat([feature_space.ix[X_testN[index][str(f)]], feature_space.ix[X_testO[index][str(f)]]], axis=0,sort=False)\n",
    "        Y_testi = pd.concat([y_testN[index][str(f)], y_testO[index][str(f)]], axis=0,sort=False)\n",
    "      \n",
    "        for nu_index, nu in enumerate(resultsVal[index][f]):\n",
    "            for gamma_index, gamma in enumerate(resultsVal[index][f][nu]): \n",
    "\n",
    "### Average Precision Score\n",
    "                val_scores[index_index,f_index,nu_index,gamma_index] = average_precision_score(Y_vali, resultsVal[index][f][nu][gamma])\n",
    "                test_scores[index_index,f_index,nu_index,gamma_index] = average_precision_score(Y_testi, resultsTest[index][f][nu][gamma])\n",
    "\n",
    "#### For python 3\n",
    "            print ('\\r{0} {1} {2}'.format(index_index, f_index, nu_index, gamma_index), end='')\n",
    "    \n",
    "print (\"\\nmean_AUC_PRCurve\",np.mean(val_scores),np.mean(test_scores))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_value of class 1 in test_score for fold=4 is 0.8722555095024943\n",
      "Class 1 for fold=4 is 0.8706169002771506\n",
      "max_value of class 2 in test_score 0.769156170880259\n",
      "Class 2  for fold=4 is 0.7657028455241435\n"
     ]
    }
   ],
   "source": [
    "fold_num = 4\n",
    "print  (\"max_value of class 1 in test_score for fold=%d is\" %fold_num, max(test_scores[0,fold_num,:,:].flatten()))\n",
    "temp_array1 = val_scores[0,fold_num,:,:]\n",
    "i_max,j_max = np.unravel_index(temp_array1.argmax(), temp_array1.shape)\n",
    "print (\"Class 1 for fold=%d is\" %fold_num, test_scores[0,fold_num, i_max,j_max])\n",
    "\n",
    "print (\"max_value of class 2 in test_score\", max(test_scores[1,fold_num,:,:].flatten()))\n",
    "temp_array2 = val_scores[1,fold_num,:,:]\n",
    "i_max,j_max = np.unravel_index(temp_array2.argmax(), temp_array2.shape)\n",
    "print (\"Class 2  for fold=%d is\" %fold_num, test_scores[1,fold_num, i_max,j_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plt.contourf(all_gamma, all_nu, test_scores[8,0,:,:])\n",
    "# plt.xscale('log', basex=2)\n",
    "# plt.yscale('log', basey=2)\n",
    "# CS = plt.colorbar(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_opt_auc = np.zeros((2,5))\n",
    "test_opt_auc = np.zeros((2,5))\n",
    "val_opt_nu = np.zeros((2,5))\n",
    "val_opt_gamma = np.zeros((2,5))\n",
    "for index_index, index in enumerate(range(1, 3)):\n",
    "    for f_index, f in enumerate(range(0, 5)):\n",
    "        temp_arr = val_scores[index_index,f_index,:,:]\n",
    "        i_max,j_max = np.unravel_index(temp_arr.argmax(), temp_arr.shape)\n",
    "#         print \"Val_max\", val_scores[index_index,f_index,i_max,j_max]\n",
    "#         print \"Test_max\", test_scores[index_index,f_index,i_max,j_max]\n",
    "        val_opt_auc[index_index,f_index] = val_scores[index_index,f_index,i_max,j_max]\n",
    "        test_opt_auc[index_index,f_index] = test_scores[index_index,f_index,i_max,j_max]\n",
    "        val_opt_nu[index_index,f_index] = all_nu[i_max]\n",
    "        val_opt_gamma[index_index,f_index] = all_gamma[j_max]\n",
    "###         print \"Val_max\", np.amax(val_scores[index_index,f_index,:,:])\n",
    "###         print \"Test_max\", np.amax(test_scores[index_index,f_index,:,:])\n",
    "\n",
    "## convert your array into a dataframe\n",
    "dfval = pd.DataFrame (val_opt_auc)\n",
    "dftest = pd.DataFrame (test_opt_auc)\n",
    "dfnu = pd.DataFrame (val_opt_nu)\n",
    "dfgamma = pd.DataFrame (val_opt_gamma)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('Final_Results_KOC_Heart_'+ privfeat +'.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "### Write optimum Parameteres\n",
    "### Write Optimum Value corresponding to Optimum parameters\n",
    "dfval.to_excel(writer, sheet_name='val_opt_KOC')\n",
    "dftest.to_excel(writer, sheet_name='test_opt_KOC')\n",
    "# dfnu.to_excel(writer, sheet_name='val_opt_KOC',startcol=9,startrow=0,index=False)\n",
    "dfnu.to_excel(writer, sheet_name='nu_opt_KOC')\n",
    "dfgamma.to_excel(writer, sheet_name='gamma_opt_KOC')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IKOC (KOC+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Workstation\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:96: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 4 0.70 10000000000.00 0.70 10000000000.00"
     ]
    }
   ],
   "source": [
    "\n",
    "all_res_val = {}\n",
    "all_res_test = {}\n",
    "X_train = {}\n",
    "X_testN = {}\n",
    "y_train = {}\n",
    "y_testN = {}\n",
    "X_val = {}  \n",
    "X_testO = {}\n",
    "y_val = {}\n",
    "y_testO = {} \n",
    "\n",
    "##### Either run below commented lines to generate 5-fold indices or load the saving indices as below\n",
    "with open('heart_5fold_index.pkl', 'rb') as f:\n",
    "        X_train, X_testN, y_train, y_testN, X_val, X_testO, y_val, y_testO = pkl.load(f)\n",
    "        \n",
    "for index_index, index in enumerate(range(1, 3)):\n",
    "    \n",
    "        ##### Either run this to generate 5-fold indices or load the saving indices as before above for loop\n",
    "        \n",
    "#     target_files = labels.query('Class == {}'.format(index)).index   ### All Target Class Files wiith indices\n",
    "#     test_indices = set(feature_space.index) - set(target_files) ### in Set for\n",
    "#     test_indices = pd.Index(test_indices) ### Convert set to index\n",
    "#     target_labels = labels.ix[target_files].Class == index ### Target data labels\n",
    "#     Outlier_labels = labels.ix[test_indices].Class == index ### Target data labels\n",
    "\n",
    "    \n",
    "#     kf = KFold(n_splits=5) ### k-fold Cross-Validation\n",
    "#     # For Target Files\n",
    "#     fold = 0\n",
    "#     X_train[index] = {}\n",
    "#     X_testN[index] = {}\n",
    "#     y_train[index] = {}\n",
    "#     y_testN[index] = {}\n",
    "#     for train_index, test_index in kf.split(target_files):\n",
    "#         #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#         X_train[index][str(fold)]=target_files[train_index]\n",
    "#         X_testN[index][str(fold)]=target_files[test_index]\n",
    "#         y_train[index][str(fold)]=target_labels[train_index]\n",
    "#         y_testN[index][str(fold)]=target_labels[test_index]\n",
    "#         fold = fold+1\n",
    "\n",
    "#     # For Outlier Files\n",
    "#     fold = 0\n",
    "#     X_val[index] = {}  \n",
    "#     X_testO[index] = {}\n",
    "#     y_val[index] = {}\n",
    "#     y_testO[index] = {}    \n",
    "#     for train_index, test_index in kf.split(test_indices):\n",
    "# #         print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#         X_val[index][str(fold)]=test_indices[train_index]\n",
    "#         X_testO[index][str(fold)]=test_indices[test_index]\n",
    "#         y_val[index][str(fold)]=Outlier_labels[train_index]\n",
    "#         y_testO[index][str(fold)]=Outlier_labels[test_index] \n",
    "#         fold=fold+1\n",
    "\n",
    "        \n",
    "### Cocatenate and Prepare final data\n",
    "    all_res_val[index] = {}\n",
    "    all_res_test[index] = {}\n",
    "    for f_index, f in enumerate(range(0, 5)):\n",
    "        all_res_val[index][f_index] = {}\n",
    "        all_res_test[index][f_index] = {}\n",
    "#        print index,f\n",
    "        X_vali = pd.concat([feature_space.ix[X_train[index][str(f)]][int(math.floor(len(X_train[index][str(f)])*0.8)):], feature_space.ix[X_val[index][str(f)]]], axis=0,sort=False)\n",
    "        Xprev_vali = pd.concat([privileged_space.ix[X_train[index][str(f)]][int(math.floor(len(X_train[index][str(f)])*0.8)):], privileged_space.ix[X_val[index][str(f)]]], axis=0,sort=False)\n",
    "        Y_vali = pd.concat([y_train[index][str(f)][int(math.floor(len(X_train[index][str(f)])*0.8)):], y_val[index][str(f)]], axis=0,sort=False)\n",
    "        X_testi = pd.concat([feature_space.ix[X_testN[index][str(f)]], feature_space.ix[X_testO[index][str(f)]]], axis=0,sort=False)\n",
    "        Y_testi = pd.concat([y_testN[index][str(f)], y_testO[index][str(f)]], axis=0,sort=False)\n",
    "#### Take 80% of samples for training and keep 20% samples for validation\n",
    "        train_id = X_train[index][str(f)][:int(math.floor(len(X_train[index][str(f)])*0.8))]\n",
    "        train_data = feature_space.ix[train_id]\n",
    "        privileged_train = privileged_space.ix[train_id]\n",
    "        train_label = y_train[index][str(f)][:int(math.floor(len(X_train[index][str(f)])*0.8))]\n",
    "        ### Parameters\n",
    "        all_nu_priv = np.linspace(0.05, 0.7, 10) ### Privileged Regularization Parameter (mu as per paper)\n",
    "        all_priv_gamma = np.logspace(-10, 10, 20) \n",
    "    \n",
    "        all_nu = np.linspace(0.05, 0.7, 10) ### Regularization parameter (C = 1/nu*no. of samples)\n",
    "        all_gamma = np.logspace(-10, 10, 20) # Power of 10 is called logspace. -10 is start point, 10 is last point ans 20 points will be generated\n",
    "      \n",
    "        for nu1 in all_nu:\n",
    "            all_res_val[index][f_index][nu1] = {}\n",
    "            all_res_test[index][f_index][nu1] = {}\n",
    "            for gamma1 in all_gamma: \n",
    "                all_res_val[index][f_index][nu1][gamma1] = {}\n",
    "                all_res_test[index][f_index][nu1][gamma1] = {}\n",
    "            #### Generate feature kernel function using gamma1     \n",
    "                features_kernel = partial(rbf_kernel, gamma=gamma1)\n",
    "                for nu in all_nu_priv:\n",
    "                    all_res_val[index][f_index][nu1][gamma1][nu] = {}\n",
    "                    all_res_test[index][f_index][nu1][gamma1][nu] = {}\n",
    "                    for gamma in all_priv_gamma:\n",
    "                        privileged_kernel = partial(rbf_kernel, gamma=gamma)\n",
    "                        model = IKOC(nu=nu1, features_kernel=features_kernel, privileged_kernel=privileged_kernel, privileged_regularization=nu)\n",
    "                        model.fit(train_data.as_matrix(), privileged_train.as_matrix().reshape(-1, 1))\n",
    "                        pred_val = model.decision_function(X_vali)\n",
    "                        all_res_val[index][f_index][nu1][gamma1][nu][gamma] = pred_val              \n",
    "                        pred_test = model.decision_function(X_testi)\n",
    "                        all_res_test[index][f_index][nu1][gamma1][nu][gamma] = pred_test             \n",
    "#                         print '\\r {0} {1} {2:.2f} {3:.2f} {4:.2f} {5:.2f}'.format(index, f, nu1, gamma1, nu, gamma),\n",
    "                        print ('\\r {0} {1} {2:.2f} {3:.2f} {4:.2f} {5:.2f}'.format(index, f, nu1, gamma1, nu, gamma), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(all_res_val, open('Heart_'+ privfeat +'_resVal_3_KOC+_5CV_CV.pkl', 'wb')) # wb denotes write bytestream\n",
    "pkl.dump(all_res_test, open('Heart_'+ privfeat +'_resTest_3_KOC+_5CV_CV.pkl', 'wb')) # wb denotes write bytestream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsVal = pkl.load(open('Heart_'+ privfeat +'_resVal_3_KOC+_5CV_CV.pkl', 'rb')) # rb denotes read bytestream\n",
    "resultsTest = pkl.load(open('Heart_'+ privfeat +'_resTest_3_KOC+_5CV_CV.pkl', 'rb')) # rb denotes read bytestream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 9 19 9 19\n",
      "mean_AUC_PRCurve 0.18267990870927384 0.48723333920806405\n"
     ]
    }
   ],
   "source": [
    "### Parameters\n",
    "all_nu_priv = np.linspace(0.05, 0.7, 10) \n",
    "all_priv_gamma = np.logspace(-10, 10, 20)\n",
    "    \n",
    "all_nu = np.linspace(0.05, 0.7, 10)\n",
    "all_gamma = np.logspace(-10, 10, 20) \n",
    "        \n",
    "val_scores = np.zeros((2, 5, all_nu.size, all_gamma.size, all_nu_priv.size, all_priv_gamma.size))\n",
    "test_scores = np.zeros((2, 5, all_nu.size, all_gamma.size, all_nu_priv.size, all_priv_gamma.size))\n",
    "\n",
    "###    Load 5-fold CV files\n",
    "with open('heart_5fold_index.pkl', 'rb') as f:\n",
    "    X_train, X_testN, y_train, y_testN, X_val, X_testO, y_val, y_testO = pkl.load(f)\n",
    "    \n",
    "for index_index, index in enumerate(range(1, 3)):\n",
    "    for f_index, f in enumerate(range(0, 5)):\n",
    "        \n",
    "#        print index,f\n",
    "        X_vali = pd.concat([feature_space.ix[X_train[index][str(f)]][int(math.floor(len(X_train[index][str(f)])*0.8)):], feature_space.ix[X_val[index][str(f)]]], axis=0,sort=False)\n",
    "        Y_vali = pd.concat([y_train[index][str(f)][int(math.floor(len(X_train[index][str(f)])*0.8)):], y_val[index][str(f)]], axis=0,sort=False)\n",
    "        X_testi = pd.concat([feature_space.ix[X_testN[index][str(f)]], feature_space.ix[X_testO[index][str(f)]]], axis=0,sort=False)\n",
    "        Y_testi = pd.concat([y_testN[index][str(f)], y_testO[index][str(f)]], axis=0,sort=False)\n",
    "      \n",
    "        for nu1_index, nu1 in enumerate(resultsVal[index][f_index]):\n",
    "            for gamma1_index, gamma1 in enumerate(resultsVal[index][f_index][nu1]):\n",
    "                for nu_index, nu in enumerate(resultsVal[index][f_index][nu1][gamma1]):\n",
    "                    for gamma_index, gamma in enumerate(resultsVal[index][f_index][nu1][gamma1][nu]): \n",
    "#                         print resultsVal[index][f_index][nu1][gamma1][nu][gamma]\n",
    "### Average Precision Score\n",
    "                        val_scores[index_index,f_index,nu1_index, gamma1_index, nu_index,gamma_index] = average_precision_score(Y_vali, resultsVal[index][f_index][nu1][gamma1][nu][gamma])\n",
    "                        test_scores[index_index,f_index,nu1_index, gamma1_index, nu_index,gamma_index] = average_precision_score(Y_testi, resultsTest[index][f_index][nu1][gamma1][nu][gamma])\n",
    "\n",
    "#### For python 3\n",
    "                        print ('\\r{0} {1} {2} {3} {4} {5}'.format(index_index, f_index, nu1_index, gamma1_index, nu_index, gamma_index), end='')\n",
    "\n",
    "\n",
    "print (\"\\nmean_AUC_PRCurve\",np.mean(val_scores),np.mean(test_scores))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_value of class 1 in test_score for fold=4 is 0.9555535803923909\n",
      "Class 1 for fold=4 is 0.9080602275599117\n",
      "max_value of class 2 in test_score for fold=4 is 0.917280592457977\n",
      "Class 2  for fold=4 is 0.8619943115468185\n"
     ]
    }
   ],
   "source": [
    "fold_num = 4\n",
    "print  (\"max_value of class 1 in test_score for fold=%d is\" %fold_num, max(test_scores[0,fold_num,:,:,:,:].flatten()))\n",
    "temp_array1 = val_scores[0,fold_num,:,:,:,:]\n",
    "i_max,j_max, k_max, l_max = np.unravel_index(temp_array1.argmax(), temp_array1.shape)\n",
    "print (\"Class 1 for fold=%d is\" %fold_num, test_scores[0,fold_num, i_max,j_max, k_max, l_max])\n",
    "\n",
    "print (\"max_value of class 2 in test_score for fold=%d is\" %fold_num, max(test_scores[1,fold_num,:,:,:,:].flatten()))\n",
    "temp_array2 = val_scores[1,fold_num,:,:,:,:]\n",
    "i_max,j_max, k_max, l_max = np.unravel_index(temp_array2.argmax(), temp_array2.shape)\n",
    "print (\"Class 2  for fold=%d is\" %fold_num, test_scores[1,fold_num, i_max,j_max, k_max, l_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select optimal parameter and save corresponding results and parameters in excel sheet\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "val_opt_auc = np.zeros((2,5))\n",
    "test_opt_auc = np.zeros((2,5))\n",
    "val_opt_nu = np.zeros((2,5))\n",
    "val_opt_gamma = np.zeros((2,5))\n",
    "val_opt_priv_nu = np.zeros((2,5))\n",
    "val_opt_priv_gamma = np.zeros((2,5))\n",
    "for index_index, index in enumerate(range(1, 3)):\n",
    "    for f_index, f in enumerate(range(0, 5)):\n",
    "        temp_arr = val_scores[index_index,f_index,:,:,:,:]\n",
    "        i_max,j_max,k_max,l_max = np.unravel_index(temp_arr.argmax(), temp_arr.shape)\n",
    "#         print \"Val_max\", val_scores[index_index,f_index,i_max,j_max]\n",
    "#         print \"Test_max\", test_scores[index_index,f_index,i_max,j_max]\n",
    "        val_opt_auc[index_index,f_index] = val_scores[index_index,f_index,i_max,j_max,k_max,l_max]\n",
    "        test_opt_auc[index_index,f_index] = test_scores[index_index,f_index,i_max,j_max,k_max,l_max]\n",
    "        val_opt_nu[index_index,f_index] = all_nu[i_max]\n",
    "        val_opt_gamma[index_index,f_index] = all_gamma[j_max]\n",
    "        val_opt_priv_nu[index_index,f_index] = all_nu_priv[k_max]\n",
    "        val_opt_priv_gamma[index_index,f_index] = all_priv_gamma[l_max]\n",
    "###         print \"Val_max\", np.amax(val_scores[index_index,f_index,:,:])\n",
    "###         print \"Test_max\", np.amax(test_scores[index_index,f_index,:,:])\n",
    "\n",
    "## convert your array into a dataframe\n",
    "dfval = pd.DataFrame (val_opt_auc)\n",
    "dftest = pd.DataFrame (test_opt_auc)\n",
    "dfnu = pd.DataFrame (val_opt_nu)\n",
    "dfgamma = pd.DataFrame (val_opt_gamma)\n",
    "dfprivnu = pd.DataFrame (val_opt_priv_nu)\n",
    "dfprivgamma = pd.DataFrame (val_opt_priv_gamma)\n",
    "\n",
    "### Load existing excel file\n",
    "book = load_workbook('Final_Results_KOC_Heart_'+ privfeat +'.xlsx')\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('Final_Results_KOC_Heart_'+ privfeat +'.xlsx', engine='openpyxl')\n",
    "writer.book = book\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "### Write Optimum Value corresponding to Optimum parameters\n",
    "dfval.to_excel(writer, sheet_name='3val_opt_KOC+')\n",
    "dftest.to_excel(writer, sheet_name='3test_opt_KOC+')\n",
    "### Write optimum Parameteres\n",
    "# dfnu.to_excel(writer, sheet_name='nu_opt',startcol=9,startrow=0)\n",
    "dfnu.to_excel(writer, sheet_name='3nu_opt+')\n",
    "dfgamma.to_excel(writer, sheet_name='3gamma_opt+')\n",
    "dfprivnu.to_excel(writer, sheet_name='3privnu_opt+')\n",
    "dfprivgamma.to_excel(writer, sheet_name='3privgamma_opt+')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save() \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
